# AI Agent Instructions for fetch-immigration-data

**Last Updated:** February 24, 2026  
**GitHub Repo:** https://github.com/v-rathod/us-immigration-datasets  
**Project Location:** `/Users/vrathod1/dev/NorthStar/fetch-immigration-data/`

---

## Active Maintenance Tasks

### 1. Auto-Commit Policy ✅ ACTIVE
**Rule:** Automatically commit ALL changes to git with descriptive messages after any modification.

**Applies to:**
- Code changes (fetch_latest.py, handlers)
- Documentation (README, FOLDER_STRUCTURE.md, data-dictionary.md)
- Configuration (sources.yaml, requirements.txt)
- Schema/structure changes
- New dataset additions

**Commit message format:**
```
Summary line (what changed)

- Bulleted details (specifics, rationale)
- Affected components
- Context for future reference
```

**Always:** `git add [files] && git commit -m "..." && git push`

---

### 2. DATA_DICTIONARY.md Auto-Maintenance ✅ ACTIVE
**Rule:** Automatically update `data-dictionary.md` whenever datasets change.

**Trigger phrases:**
- "new data added"
- "rerun field inventory"
- "update dictionary"
- "refresh schema notes"
- "reflect latest downloads"

**Actions:**
1. Scan `downloads/` recursively for changes
2. Detect new dataset groups, fiscal years, file types, schema versions
3. Update DATA_DICTIONARY.md by:
   - Adding new subsections for new datasets
   - Updating existing sections (fiscal years, formats, URLs)
   - Refreshing "Key fields to extract" when structures change
   - Documenting new caveats, schema transitions, record layouts
   - Updating joins when new fields appear (SOC, country, employer, geo)
4. Append update date + change summary at top
5. Never delete existing sections—only expand or correct
6. Maintain consistent markdown formatting
7. Generate field inventory for new files (columns, types, row counts, nulls) in Appendix A
8. Ask only when ambiguous (join keys, category mappings)—otherwise use safe defaults
9. Auto-commit after update

---

## Project Context

### Repository Structure
```
/Users/vrathod1/dev/NorthStar/
├── fetch-immigration-data/          ← THIS PROJECT (data collection)
│   ├── fetch_latest.py              (3,309 lines, main downloader)
│   ├── sources.yaml                 (17 data source configs)
│   ├── requirements.txt             (6 dependencies)
│   ├── data-dictionary.md           (15+ datasets documented)
│   ├── FOLDER_STRUCTURE.md          (1,230 files, 210 folders)
│   ├── downloads/                   (3-5 GB, gitignored)
│   └── .venv/                       (Python 3.12, gitignored)
└── immigration-model-builder/       ← PROJECT 2 (ML models)
```

### Current Status (Feb 24, 2026)
- ✅ 5/5 new data sources operational (TRAC and USCIS Processing Times removed)
- ✅ 1,226 files across 207 folders
- ✅ Incremental downloads working (manifest-based deduplication)
- ✅ All dependencies installed (selenium, webdriver-manager, PyPDF2)
- ✅ data-dictionary.md created (commit d35ac22)
- ✅ DOS Waiting List source complete (2023 data)

### Data Sources (15 active)
**Original 10 (legacy):**
1. Visa Bulletin (monthly PDFs, 2011-2026)
2. Visa Statistics (monthly IV issuances, 2017-2025)
3. Visa Annual Reports (FY2015-2024)
4. USCIS Immigration (I-140/I-485/I-765, 1991-2026)
5. LCA (H-1B, FY2008-2026)
6. PERM (FY2008-2026)
7. NIV Statistics
8. BLS (employment)
9. ACS (census)
10. WARN (layoffs)

**New 7 (added Feb 2026):**
11. ✅ DOL Record Layouts (PERM/LCA schemas, FY2011+FY2020-2026)
12. ✅ BLS OEWS (wage percentiles, 2023-2024)
13. ✅ USCIS H-1B Employer Hub (FY2010-2023)
14. ✅ DOS Numerical Limits (FY2025)
15. ✅ Codebooks (SOC crosswalk, country codes, EB categories)
16. ✅ DOS Waiting List (annual backlog PDFs + parsed CSVs)

**Removed:**
- ❌ USCIS Processing Times (Cloudflare-protected SPA, data not extractable)
- ❌ TRAC Immigration Data (requires paid subscription)

---

## Key Conventions

### File Organization
- **Raw data:** `downloads/{SOURCE}/{YEAR}/` or `downloads/{SOURCE}/raw/{YEAR}/`
- **Parsed data:** `downloads/{SOURCE}/parsed/{YEAR}/`
- **Manifest:** `downloads/_manifest.json` (URL+path+hash tracking)

### Incremental Downloads
- **Before download:** Check `is_file_in_manifest(url, path, manifest)`
- **If exists:** Skip with log message "Already have {file}, skipping"
- **If new:** Download, compute hash, add to manifest
- **Result:** Only downloads delta/new data on subsequent runs

### Dependencies (requirements.txt)
```
requests==2.32.3
beautifulsoup4==4.12.3
pyyaml==6.0.2
selenium==4.27.1          # Cloudflare bypass
webdriver-manager==4.0.2  # ChromeDriver auto-install
PyPDF2==3.0.1             # PDF parsing
```

### Virtual Environment
- **Location:** `.venv/` (gitignored)
- **Setup:** `python3 -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt`
- **Note:** Recreate if project moved (hardcoded paths in activate script)

---

## Important Implementation Details

### DOS Waiting List (lines 2523-2770)
- Tests multiple URL patterns per fiscal year
- Direct PDF downloads from travel.state.gov CDN
- PyPDF2 parsing: fiscal_year, category, country, count
- Yearly reports (skip existing years)

### LCA/PERM (lines 1300-2270)
- Major schema change in FY2020 (FLAG era)
- Always reference record layouts for accurate parsing
- Quarterly + annual files

---

## Git Workflow

### .gitignore (what's NOT tracked)
```
.venv/                    # Virtual environment
downloads/                # Data files (too large)
exports/                  # Exports
*.log, fetch_log.txt      # Logs
test_*.py, check_*.py     # Temporary test scripts
__pycache__/              # Python cache
.DS_Store                 # macOS metadata
```

### What IS tracked
- All Python code (fetch_latest.py)
- All documentation (README.md, FOLDER_STRUCTURE.md, data-dictionary.md, .ai-instructions.md)
- Configuration (sources.yaml, requirements.txt, .gitignore)
- Git metadata (.git/)

---

## Commands Reference

### Daily Operations
```bash
# Activate environment
cd /Users/vrathod1/dev/NorthStar/fetch-immigration-data
source .venv/bin/activate

# Run full download (incremental)
python fetch_latest.py sources.yaml

# Check manifest
python -c "from fetch_latest import load_manifest; from pathlib import Path; m = load_manifest(Path('downloads')); print(f'{len(m.get(\"files_by_url\", {}))} files tracked')"

# Check git status
git status
```

### Maintenance
```bash
# Update dictionary (triggers auto-update)
# Just say: "update dictionary" or "reflect latest downloads"

# Manual commit (if auto-commit disabled)
git add .
git commit -m "Description"
git push
```

---

## User Preferences

### Communication
- Brief, direct responses (1-3 sentences for simple questions)
- No emojis unless requested
- No unnecessary framing ("Here's the answer:", "I will now...")
- Confirm actions briefly after completion

### Development
- Never modify existing code without explicit request
- Follow existing folder/manifest conventions
- Do NOT re-download existing data
- Always use relative paths (not absolute)

### Documentation
- Keep FOLDER_STRUCTURE.md updated (file counts, new sources)
- Keep data-dictionary.md as functional contract for downstream projects
- Use markdown links for file references: [path/file.ts](path/file.ts#L10)
- Update dates on all docs when changed

---

## Next Steps (When User Returns)

1. **Check for new data:** "Any new files downloaded since last session?"
2. **Update docs if needed:** "update dictionary" (triggers auto-maintenance)
3. **Verify incremental downloads:** Confirm manifest working correctly
4. **Project 2 coordination:** Ensure data-dictionary.md reflects current state for model-builder

---

## Troubleshooting

**Virtual environment not working after move:**
```bash
rm -rf .venv
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

**Cloudflare blocking USCIS:**
- Already handled with Selenium (10s wait, anti-detection)
- If fails: Increase wait time in line 2357

**DOS PDF 404 errors:**
- Only 2023 currently available
- Older years may use different URL patterns
- Handler tries multiple patterns automatically

---

*This file serves as persistent context for AI agents working with this repository. Update whenever maintenance tasks, conventions, or project structure changes.*
